df_test_preds_selected$mean_pred <- rowMeans(df_test_preds_list[[count]])
df_test_preds_selected$expl <- df_test$x
df_test_preds_selected$fx <- df_test$fx
df_test_preds_selected_pivot1 <- df_test_preds_selected %>%
pivot_longer(cols = starts_with("X"), names_to = "rep_data", values_to = "preds")
}
# df_test_preds_list <- bvt_calc_lasso[[3]]
df_test_preds_list2 <- lapply(df_test_preds_list, function(e) data.frame(cbind(e,
mean_pred = rowmeans(e),
expl = df_test$x,
fx = df_test$fx)))
df_test_preds_selected_pivot <- lapply(df_test_preds_list2, function(e) e %>% pivot_longer(cols = starts_with("V"), names_to = "rep_data", values_to = "preds"))
df3_list <- list_rbind(df_test_preds_selected_pivot)
df3_list$parameter_val <- rep(results$parameter_values, each = 50000)
return(list(results, parameter_values, df_test_preds_list, noise_sd,
training_data[1:training_data_size,], df_test, df_training_ys, df3_list))
}
fit_polynomial <- function(x, y, degree, df_test) {
m <- lm(y ~ poly(x, degree = degree, raw = TRUE))
return(list(y_hat_training = m$fitted.values, y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_spline <- function(x, y, df, df_test) {
m <- smooth.spline(x = x, y = y, df = df)
return(list(y_hat_training = predict(m, x = x)$y, y_hat_test = predict(m, x = df_test$x)$y))
}
fit_knn <- function(x, y, k, df_test) {
# list(
#   predict = function(new_x) {
#     Rfast::knn(xnew = matrix(new_x), y = y, x = matrix(x), k = k, type = "R")
#   }
# )
return(list(y_hat_training = Rfast::knn(xnew = matrix(x), y = y,
x = matrix(x), k = k, type = "R"),
y_hat_test = Rfast::knn(xnew = matrix(df_test$x), y = y,
x = matrix(x), k = k, type = "R")))
}
fit_tree <- function(x, y, depth, df_test) {
m <- rpart(y ~ x, method = "anova", control = rpart.control(cp = 0, xval = 0, minbucket = 1, maxdepth = depth))
return(list(y_hat_training = predict(m, newdata = data.frame(x)), y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_lasso <- function(x, y, lambda, df_test) {
poly_degree <- 5
x_rep_mat <- model.matrix(~ poly(x, degree = poly_degree, raw = TRUE))[,-1]
m <- glmnet(x = x_rep_mat, y = y, family = "gaussian", alpha = 0, lambda = 10^lambda)
return(list(y_hat_training = predict(m, newx = x_rep_mat),
y_hat_test = predict(m, newx = model.matrix(~ poly(df_test$x, degree = poly_degree, raw = TRUE))[,-1])))
}
dataset <- 3
#  1500 = 500 TRAINING + 1000 TEST, CHANGE THIS IF OPTIONS CHANGE LATER
set.seed(3)
# x1 <- runif(num_total_points, -5, 10)
# x2 <- runif(num_total_points, 0, 1)
x3 <- runif(num_total_points, -5, 5)
# fx1 <- ifelse(0.1 * (x1^2) < 3, 0.1 * (x1^2), 3)
# fx2 <- sin(12 * (x2 + 0.2)) / (x2 + 0.2)
fx3 <- a + (b * x3^2) + (c * x3^3)
set.seed(3); noise <- matrix(rnorm(num_total_points * (num_rep_sets_reg + 1), mean = 0, sd = 1),
nrow = num_total_points, ncol = num_rep_sets_reg + 1)
# par(mfcol = c(1, 2))
# plot(x1, fx1); plot(x1, y1[,51])
# plot(x2, fx2); plot(x2, y2[,51])
# plot(x3, fx3); plot(x3, y3[,51])
# par(mfcol = c(1,1))
# e_test <- rnorm(num_test_points_reg, mean = 0, sd = noise_sd)
# y_test <- fx[501:1500] + e_test    # say, original y
size <- c(100, 300, 500)
error_noise <- c(1, 5, 10)
size_counter <- 3; error_noise_counter <- 1
# for(size_counter in seq_along(size))
# {
#   for(error_noise_counter in seq_along(error_noise))
#   {
# y1 <- fx1 + noise*error_noise[error_noise_counter]
# y2 <- fx2 + noise*error_noise[error_noise_counter]
y3 <- fx3 + noise*error_noise[error_noise_counter]
# change x1, x2, x3 and y1, y2, y3 as per dataset
df_training_full <- data.frame(x = x3[1:num_training_points], fx = fx3[1:num_training_points], y_orig = y3[1:num_training_points,1])
df_test <- data.frame(x = x3[(num_training_points+1):num_total_points], fx = fx3[(num_training_points+1):num_total_points], y_orig = y3[(num_training_points+1):num_total_points,51])
bvt_calc_polynomial <- calculate_bias_variance(model_type = "polynomial",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = 1:8,
fit_function = fit_polynomial,
noise_sd = error_noise[error_noise_counter])
bvt_calc_spline <- calculate_bias_variance(model_type = "spline",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 100, 13),
fit_function = fit_spline,
noise_sd = error_noise[error_noise_counter])
bvt_calc_knn_reg <- calculate_bias_variance(model_type = "knn_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(1, 15, 2),
fit_function = fit_knn,
noise_sd = error_noise[error_noise_counter])
bvt_calc_tree_reg <- calculate_bias_variance(model_type = "tree_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 16, 2),
fit_function = fit_tree,
noise_sd = error_noise[error_noise_counter])
bvt_calc_lasso <- calculate_bias_variance(model_type = "lasso",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(-4,3,1),
fit_function = fit_lasso,
noise_sd = error_noise[error_noise_counter])
sum(bvt_calc_polynomial[[5]]==bvt_calc_spline[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_knn_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_tree_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_lasso[[5]])
sum(bvt_calc_polynomial[[6]]==bvt_calc_spline[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_knn_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_tree_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_lasso[[6]])
sum(bvt_calc_polynomial[[7]]==bvt_calc_spline[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_knn_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_tree_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_lasso[[7]])
setwd(sprintf("C:/Users/chakraba/OneDrive - Lawrence University/Research/BVT app/app/regression-latest/data/BVTappRegression/d%s_n%s_e%s",
dataset, size[size_counter], error_noise[error_noise_counter]))
getwd()
write.csv(bvt_calc_polynomial[[5]], "df_training_full.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[6]], "df_test.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[7]], "df_training_ys.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[1]], "results_polynomial.csv", row.names = FALSE)
write.csv(bvt_calc_spline[[1]], "results_spline.csv", row.names = FALSE)
write.csv(bvt_calc_knn_reg[[1]], "results_knn_reg.csv", row.names = FALSE)
write.csv(bvt_calc_tree_reg[[1]], "results_tree_reg.csv", row.names = FALSE)
write.csv(bvt_calc_lasso[[1]], "results_lasso.csv", row.names = FALSE)
write_parquet(bvt_calc_polynomial[[8]], "df3_list_polynomial.parquet")
write_parquet(bvt_calc_spline[[8]], "df3_list_spline.parquet")
write_parquet(bvt_calc_knn_reg[[8]], "df3_list_knn_reg.parquet")
write_parquet(bvt_calc_tree_reg[[8]], "df3_list_tree_reg.parquet")
write_parquet(bvt_calc_lasso[[8]], "df3_list_lasso.parquet")
rm(list = ls())
##################################################################################################
num_test_points_reg <- 1000      # number of test data points
num_rep_sets_reg <- 50                 # number of replicated datasets
# sample_reps <- sample(1:num_rep_sets_reg, 20)
num_training_points <- 500
num_total_points <- num_training_points + num_test_points_reg
a <- 3; b <- 0.87; c <- 0.5
calculate_bias_variance <- function(model_type, training_data, training_data_size, test_data, parameter_values, fit_function, noise_sd) {
# df_test <- df_data[[2]]
# df_training_xs <- df_data[[3]]
# df_training_ys <- df_data[[4]]
# noise_sd <- df_data[[5]]
df_test <- test_data
df_training_xs <- training_data$x[1:training_data_size]
df_training_ys <- y3[1:training_data_size, -(num_rep_sets_reg+1)]   # change as per y1, y2, y3
# num_training_points <- length(df_training_xs)
df_test_preds <- matrix(nrow = num_test_points_reg, ncol = num_rep_sets_reg)
train_mse <- numeric(num_rep_sets_reg)
df_test_preds_list <- list()
bias_sq <- numeric(length(parameter_values))
variance <- numeric(length(parameter_values))
training_mse <- numeric(length(parameter_values))
for (count in seq_along(parameter_values)) {
param <- parameter_values[count]
for (j in seq_len(num_rep_sets_reg)) {
y_rep <- df_training_ys[, j]
x_rep <- df_training_xs
# Fit the model using the provided fit_function
model <- fit_function(x_rep, y_rep, param, df_test)
# y_hat_training <- predict(model, x_rep)
# y_hat_test <- predict(model, df_test$x)
y_hat_training <- model$y_hat_training
y_hat_test <- model$y_hat_test
df_test_preds[, j] <- y_hat_test
train_mse[j] <- mean((y_rep - y_hat_training)^2)
}
# Bias, variance, and MSE calculations
E_y_hat <- rowMeans(df_test_preds)
V_y_hat <- Rfast::rowVars(df_test_preds)
bias_squared <- (E_y_hat - df_test$fx)^2
training_mse[count] <- mean(train_mse)
bias_sq[count] <- mean(bias_squared)
variance[count] <- mean(V_y_hat)
df_test_preds_list[[count]] <- df_test_preds
}
results <- data.frame(
parameter_values = factor(parameter_values),
Bias_sq = bias_sq,
Variance = variance,
Test = bias_sq + variance + (noise_sd^2),
Training = training_mse
)
for(count in seq_along(parameter_values))
{
df_test_preds_selected <- data.frame(df_test_preds_list[[count]])
df_test_preds_selected$mean_pred <- rowMeans(df_test_preds_list[[count]])
df_test_preds_selected$expl <- df_test$x
df_test_preds_selected$fx <- df_test$fx
df_test_preds_selected_pivot1 <- df_test_preds_selected %>%
pivot_longer(cols = starts_with("X"), names_to = "rep_data", values_to = "preds")
}
# df_test_preds_list <- bvt_calc_lasso[[3]]
df_test_preds_list2 <- lapply(df_test_preds_list, function(e) data.frame(cbind(e,
mean_pred = rowmeans(e),
expl = df_test$x,
fx = df_test$fx)))
df_test_preds_selected_pivot <- lapply(df_test_preds_list2, function(e) e %>% pivot_longer(cols = starts_with("V"), names_to = "rep_data", values_to = "preds"))
df3_list <- list_rbind(df_test_preds_selected_pivot)
df3_list$parameter_val <- rep(results$parameter_values, each = 50000)
return(list(results, parameter_values, df_test_preds_list, noise_sd,
training_data[1:training_data_size,], df_test, df_training_ys, df3_list))
}
fit_polynomial <- function(x, y, degree, df_test) {
m <- lm(y ~ poly(x, degree = degree, raw = TRUE))
return(list(y_hat_training = m$fitted.values, y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_spline <- function(x, y, df, df_test) {
m <- smooth.spline(x = x, y = y, df = df)
return(list(y_hat_training = predict(m, x = x)$y, y_hat_test = predict(m, x = df_test$x)$y))
}
fit_knn <- function(x, y, k, df_test) {
# list(
#   predict = function(new_x) {
#     Rfast::knn(xnew = matrix(new_x), y = y, x = matrix(x), k = k, type = "R")
#   }
# )
return(list(y_hat_training = Rfast::knn(xnew = matrix(x), y = y,
x = matrix(x), k = k, type = "R"),
y_hat_test = Rfast::knn(xnew = matrix(df_test$x), y = y,
x = matrix(x), k = k, type = "R")))
}
fit_tree <- function(x, y, depth, df_test) {
m <- rpart(y ~ x, method = "anova", control = rpart.control(cp = 0, xval = 0, minbucket = 1, maxdepth = depth))
return(list(y_hat_training = predict(m, newdata = data.frame(x)), y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_lasso <- function(x, y, lambda, df_test) {
poly_degree <- 5
x_rep_mat <- model.matrix(~ poly(x, degree = poly_degree, raw = TRUE))[,-1]
m <- glmnet(x = x_rep_mat, y = y, family = "gaussian", alpha = 0, lambda = 10^lambda)
return(list(y_hat_training = predict(m, newx = x_rep_mat),
y_hat_test = predict(m, newx = model.matrix(~ poly(df_test$x, degree = poly_degree, raw = TRUE))[,-1])))
}
dataset <- 3
#  1500 = 500 TRAINING + 1000 TEST, CHANGE THIS IF OPTIONS CHANGE LATER
set.seed(3)
# x1 <- runif(num_total_points, -5, 10)
# x2 <- runif(num_total_points, 0, 1)
x3 <- runif(num_total_points, -5, 5)
# fx1 <- ifelse(0.1 * (x1^2) < 3, 0.1 * (x1^2), 3)
# fx2 <- sin(12 * (x2 + 0.2)) / (x2 + 0.2)
fx3 <- a + (b * x3^2) + (c * x3^3)
set.seed(3); noise <- matrix(rnorm(num_total_points * (num_rep_sets_reg + 1), mean = 0, sd = 1),
nrow = num_total_points, ncol = num_rep_sets_reg + 1)
# par(mfcol = c(1, 2))
# plot(x1, fx1); plot(x1, y1[,51])
# plot(x2, fx2); plot(x2, y2[,51])
# plot(x3, fx3); plot(x3, y3[,51])
# par(mfcol = c(1,1))
# e_test <- rnorm(num_test_points_reg, mean = 0, sd = noise_sd)
# y_test <- fx[501:1500] + e_test    # say, original y
size <- c(100, 300, 500)
error_noise <- c(1, 5, 10)
size_counter <- 3; error_noise_counter <- 2
# for(size_counter in seq_along(size))
# {
#   for(error_noise_counter in seq_along(error_noise))
#   {
# y1 <- fx1 + noise*error_noise[error_noise_counter]
# y2 <- fx2 + noise*error_noise[error_noise_counter]
y3 <- fx3 + noise*error_noise[error_noise_counter]
# change x1, x2, x3 and y1, y2, y3 as per dataset
df_training_full <- data.frame(x = x3[1:num_training_points], fx = fx3[1:num_training_points], y_orig = y3[1:num_training_points,1])
df_test <- data.frame(x = x3[(num_training_points+1):num_total_points], fx = fx3[(num_training_points+1):num_total_points], y_orig = y3[(num_training_points+1):num_total_points,51])
bvt_calc_polynomial <- calculate_bias_variance(model_type = "polynomial",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = 1:8,
fit_function = fit_polynomial,
noise_sd = error_noise[error_noise_counter])
bvt_calc_spline <- calculate_bias_variance(model_type = "spline",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 100, 13),
fit_function = fit_spline,
noise_sd = error_noise[error_noise_counter])
bvt_calc_knn_reg <- calculate_bias_variance(model_type = "knn_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(1, 15, 2),
fit_function = fit_knn,
noise_sd = error_noise[error_noise_counter])
bvt_calc_tree_reg <- calculate_bias_variance(model_type = "tree_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 16, 2),
fit_function = fit_tree,
noise_sd = error_noise[error_noise_counter])
bvt_calc_lasso <- calculate_bias_variance(model_type = "lasso",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(-4,3,1),
fit_function = fit_lasso,
noise_sd = error_noise[error_noise_counter])
sum(bvt_calc_polynomial[[5]]==bvt_calc_spline[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_knn_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_tree_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_lasso[[5]])
sum(bvt_calc_polynomial[[6]]==bvt_calc_spline[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_knn_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_tree_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_lasso[[6]])
sum(bvt_calc_polynomial[[7]]==bvt_calc_spline[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_knn_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_tree_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_lasso[[7]])
setwd(sprintf("C:/Users/chakraba/OneDrive - Lawrence University/Research/BVT app/app/regression-latest/data/BVTappRegression/d%s_n%s_e%s",
dataset, size[size_counter], error_noise[error_noise_counter]))
getwd()
write.csv(bvt_calc_polynomial[[5]], "df_training_full.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[6]], "df_test.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[7]], "df_training_ys.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[1]], "results_polynomial.csv", row.names = FALSE)
write.csv(bvt_calc_spline[[1]], "results_spline.csv", row.names = FALSE)
write.csv(bvt_calc_knn_reg[[1]], "results_knn_reg.csv", row.names = FALSE)
write.csv(bvt_calc_tree_reg[[1]], "results_tree_reg.csv", row.names = FALSE)
write.csv(bvt_calc_lasso[[1]], "results_lasso.csv", row.names = FALSE)
write_parquet(bvt_calc_polynomial[[8]], "df3_list_polynomial.parquet")
write_parquet(bvt_calc_spline[[8]], "df3_list_spline.parquet")
write_parquet(bvt_calc_knn_reg[[8]], "df3_list_knn_reg.parquet")
write_parquet(bvt_calc_tree_reg[[8]], "df3_list_tree_reg.parquet")
write_parquet(bvt_calc_lasso[[8]], "df3_list_lasso.parquet")
rm(list = ls())
##################################################################################################
num_test_points_reg <- 1000      # number of test data points
num_rep_sets_reg <- 50                 # number of replicated datasets
# sample_reps <- sample(1:num_rep_sets_reg, 20)
num_training_points <- 500
num_total_points <- num_training_points + num_test_points_reg
a <- 3; b <- 0.87; c <- 0.5
calculate_bias_variance <- function(model_type, training_data, training_data_size, test_data, parameter_values, fit_function, noise_sd) {
# df_test <- df_data[[2]]
# df_training_xs <- df_data[[3]]
# df_training_ys <- df_data[[4]]
# noise_sd <- df_data[[5]]
df_test <- test_data
df_training_xs <- training_data$x[1:training_data_size]
df_training_ys <- y3[1:training_data_size, -(num_rep_sets_reg+1)]   # change as per y1, y2, y3
# num_training_points <- length(df_training_xs)
df_test_preds <- matrix(nrow = num_test_points_reg, ncol = num_rep_sets_reg)
train_mse <- numeric(num_rep_sets_reg)
df_test_preds_list <- list()
bias_sq <- numeric(length(parameter_values))
variance <- numeric(length(parameter_values))
training_mse <- numeric(length(parameter_values))
for (count in seq_along(parameter_values)) {
param <- parameter_values[count]
for (j in seq_len(num_rep_sets_reg)) {
y_rep <- df_training_ys[, j]
x_rep <- df_training_xs
# Fit the model using the provided fit_function
model <- fit_function(x_rep, y_rep, param, df_test)
# y_hat_training <- predict(model, x_rep)
# y_hat_test <- predict(model, df_test$x)
y_hat_training <- model$y_hat_training
y_hat_test <- model$y_hat_test
df_test_preds[, j] <- y_hat_test
train_mse[j] <- mean((y_rep - y_hat_training)^2)
}
# Bias, variance, and MSE calculations
E_y_hat <- rowMeans(df_test_preds)
V_y_hat <- Rfast::rowVars(df_test_preds)
bias_squared <- (E_y_hat - df_test$fx)^2
training_mse[count] <- mean(train_mse)
bias_sq[count] <- mean(bias_squared)
variance[count] <- mean(V_y_hat)
df_test_preds_list[[count]] <- df_test_preds
}
results <- data.frame(
parameter_values = factor(parameter_values),
Bias_sq = bias_sq,
Variance = variance,
Test = bias_sq + variance + (noise_sd^2),
Training = training_mse
)
for(count in seq_along(parameter_values))
{
df_test_preds_selected <- data.frame(df_test_preds_list[[count]])
df_test_preds_selected$mean_pred <- rowMeans(df_test_preds_list[[count]])
df_test_preds_selected$expl <- df_test$x
df_test_preds_selected$fx <- df_test$fx
df_test_preds_selected_pivot1 <- df_test_preds_selected %>%
pivot_longer(cols = starts_with("X"), names_to = "rep_data", values_to = "preds")
}
# df_test_preds_list <- bvt_calc_lasso[[3]]
df_test_preds_list2 <- lapply(df_test_preds_list, function(e) data.frame(cbind(e,
mean_pred = rowmeans(e),
expl = df_test$x,
fx = df_test$fx)))
df_test_preds_selected_pivot <- lapply(df_test_preds_list2, function(e) e %>% pivot_longer(cols = starts_with("V"), names_to = "rep_data", values_to = "preds"))
df3_list <- list_rbind(df_test_preds_selected_pivot)
df3_list$parameter_val <- rep(results$parameter_values, each = 50000)
return(list(results, parameter_values, df_test_preds_list, noise_sd,
training_data[1:training_data_size,], df_test, df_training_ys, df3_list))
}
fit_polynomial <- function(x, y, degree, df_test) {
m <- lm(y ~ poly(x, degree = degree, raw = TRUE))
return(list(y_hat_training = m$fitted.values, y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_spline <- function(x, y, df, df_test) {
m <- smooth.spline(x = x, y = y, df = df)
return(list(y_hat_training = predict(m, x = x)$y, y_hat_test = predict(m, x = df_test$x)$y))
}
fit_knn <- function(x, y, k, df_test) {
# list(
#   predict = function(new_x) {
#     Rfast::knn(xnew = matrix(new_x), y = y, x = matrix(x), k = k, type = "R")
#   }
# )
return(list(y_hat_training = Rfast::knn(xnew = matrix(x), y = y,
x = matrix(x), k = k, type = "R"),
y_hat_test = Rfast::knn(xnew = matrix(df_test$x), y = y,
x = matrix(x), k = k, type = "R")))
}
fit_tree <- function(x, y, depth, df_test) {
m <- rpart(y ~ x, method = "anova", control = rpart.control(cp = 0, xval = 0, minbucket = 1, maxdepth = depth))
return(list(y_hat_training = predict(m, newdata = data.frame(x)), y_hat_test = predict(m, newdata = data.frame(x = df_test$x))))
}
fit_lasso <- function(x, y, lambda, df_test) {
poly_degree <- 5
x_rep_mat <- model.matrix(~ poly(x, degree = poly_degree, raw = TRUE))[,-1]
m <- glmnet(x = x_rep_mat, y = y, family = "gaussian", alpha = 0, lambda = 10^lambda)
return(list(y_hat_training = predict(m, newx = x_rep_mat),
y_hat_test = predict(m, newx = model.matrix(~ poly(df_test$x, degree = poly_degree, raw = TRUE))[,-1])))
}
dataset <- 3
#  1500 = 500 TRAINING + 1000 TEST, CHANGE THIS IF OPTIONS CHANGE LATER
set.seed(3)
# x1 <- runif(num_total_points, -5, 10)
# x2 <- runif(num_total_points, 0, 1)
x3 <- runif(num_total_points, -5, 5)
# fx1 <- ifelse(0.1 * (x1^2) < 3, 0.1 * (x1^2), 3)
# fx2 <- sin(12 * (x2 + 0.2)) / (x2 + 0.2)
fx3 <- a + (b * x3^2) + (c * x3^3)
set.seed(3); noise <- matrix(rnorm(num_total_points * (num_rep_sets_reg + 1), mean = 0, sd = 1),
nrow = num_total_points, ncol = num_rep_sets_reg + 1)
# par(mfcol = c(1, 2))
# plot(x1, fx1); plot(x1, y1[,51])
# plot(x2, fx2); plot(x2, y2[,51])
# plot(x3, fx3); plot(x3, y3[,51])
# par(mfcol = c(1,1))
# e_test <- rnorm(num_test_points_reg, mean = 0, sd = noise_sd)
# y_test <- fx[501:1500] + e_test    # say, original y
size <- c(100, 300, 500)
error_noise <- c(1, 5, 10)
size_counter <- 3; error_noise_counter <- 3
# for(size_counter in seq_along(size))
# {
#   for(error_noise_counter in seq_along(error_noise))
#   {
# y1 <- fx1 + noise*error_noise[error_noise_counter]
# y2 <- fx2 + noise*error_noise[error_noise_counter]
y3 <- fx3 + noise*error_noise[error_noise_counter]
# change x1, x2, x3 and y1, y2, y3 as per dataset
df_training_full <- data.frame(x = x3[1:num_training_points], fx = fx3[1:num_training_points], y_orig = y3[1:num_training_points,1])
df_test <- data.frame(x = x3[(num_training_points+1):num_total_points], fx = fx3[(num_training_points+1):num_total_points], y_orig = y3[(num_training_points+1):num_total_points,51])
bvt_calc_polynomial <- calculate_bias_variance(model_type = "polynomial",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = 1:8,
fit_function = fit_polynomial,
noise_sd = error_noise[error_noise_counter])
bvt_calc_spline <- calculate_bias_variance(model_type = "spline",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 100, 13),
fit_function = fit_spline,
noise_sd = error_noise[error_noise_counter])
bvt_calc_knn_reg <- calculate_bias_variance(model_type = "knn_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(1, 15, 2),
fit_function = fit_knn,
noise_sd = error_noise[error_noise_counter])
bvt_calc_tree_reg <- calculate_bias_variance(model_type = "tree_reg",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(2, 16, 2),
fit_function = fit_tree,
noise_sd = error_noise[error_noise_counter])
bvt_calc_lasso <- calculate_bias_variance(model_type = "lasso",
training_data = df_training_full,
training_data_size = size[size_counter],
test_data = df_test,
parameter_values = seq(-4,3,1),
fit_function = fit_lasso,
noise_sd = error_noise[error_noise_counter])
sum(bvt_calc_polynomial[[5]]==bvt_calc_spline[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_knn_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_tree_reg[[5]]); sum(bvt_calc_polynomial[[5]]==bvt_calc_lasso[[5]])
sum(bvt_calc_polynomial[[6]]==bvt_calc_spline[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_knn_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_tree_reg[[6]]); sum(bvt_calc_polynomial[[6]]==bvt_calc_lasso[[6]])
sum(bvt_calc_polynomial[[7]]==bvt_calc_spline[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_knn_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_tree_reg[[7]]); sum(bvt_calc_polynomial[[7]]==bvt_calc_lasso[[7]])
setwd(sprintf("C:/Users/chakraba/OneDrive - Lawrence University/Research/BVT app/app/regression-latest/data/BVTappRegression/d%s_n%s_e%s",
dataset, size[size_counter], error_noise[error_noise_counter]))
getwd()
write.csv(bvt_calc_polynomial[[5]], "df_training_full.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[6]], "df_test.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[7]], "df_training_ys.csv", row.names = FALSE)
write.csv(bvt_calc_polynomial[[1]], "results_polynomial.csv", row.names = FALSE)
write.csv(bvt_calc_spline[[1]], "results_spline.csv", row.names = FALSE)
write.csv(bvt_calc_knn_reg[[1]], "results_knn_reg.csv", row.names = FALSE)
write.csv(bvt_calc_tree_reg[[1]], "results_tree_reg.csv", row.names = FALSE)
write.csv(bvt_calc_lasso[[1]], "results_lasso.csv", row.names = FALSE)
write_parquet(bvt_calc_polynomial[[8]], "df3_list_polynomial.parquet")
write_parquet(bvt_calc_spline[[8]], "df3_list_spline.parquet")
write_parquet(bvt_calc_knn_reg[[8]], "df3_list_knn_reg.parquet")
write_parquet(bvt_calc_tree_reg[[8]], "df3_list_tree_reg.parquet")
write_parquet(bvt_calc_lasso[[8]], "df3_list_lasso.parquet")
rm(list = ls())
runApp('C:/Users/chakraba/OneDrive - Lawrence University/Research/BVT app/app/regression-latest')
runApp('C:/Users/chakraba/OneDrive - Lawrence University/Research/BVT app/app/regression-latest')
rsconnect::setAccountInfo(name='efriedlander', token='17AB86DD6AC747D5C9F63D6859126735', secret='G77GPVbTFfUHEqXfScVnW4waFa+ES0U4gbZrPzFa')
